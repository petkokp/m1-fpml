{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron - from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "# plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 We provide a simple data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we show how numpy works, building only one of the 2 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1=(1,1)\n",
    "sigma1=((2, 0.01))\n",
    "# points distributed according to a multi-variate Gaussian distribution : \n",
    "X1 = np.random.normal( mu1, sigma1,(100,2))  \n",
    "plt.scatter(X1[:,0], X1[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make another blob and stack the two blobs (classes) together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data set 3\n",
    "N=100\n",
    "N1=N//4\n",
    "N2=N-N1\n",
    "D=2\n",
    "np.random.seed(42)\n",
    "\n",
    "# parameters for the 1st blob of points\n",
    "mu1=(0.3,0.3)\n",
    "sigma1=((1, 0.3))\n",
    "X1 = np.random.normal( mu1, sigma1,(N1,D))\n",
    "\n",
    "# parameters for the 2nd blob of points\n",
    "mu2=(-1,-1)\n",
    "sigma2=((2, 0.5))\n",
    "X2 = np.random.normal( mu2, sigma2,(N2,D))\n",
    "\n",
    "# the two blobs are merged, and labels  +1/-1  are assigned\n",
    "Xraw = np.concatenate( (X1, X2) )\n",
    "T = np.concatenate( (np.ones(N1), -np.ones(N2)) ) # .reshape(N,1)\n",
    "X = Xraw.copy() # then X will be the extended vector, with the ones added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Preliminary: trick of the ones\n",
    "\n",
    "It's advised to augment the dataset with ones, \"as usual\", to account for the bias term in a more elegant and practical way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  np.hstack((np.ones((N,1)), X))  # extended vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple plot of the 2D data (after adding the 1's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([0,0],[0,0], color='k', marker=\"+\")\n",
    "plt.axhline(0, ls=\"--\", color='k')\n",
    "plt.axvline(0, ls=\"--\", color='k')\n",
    "plt.scatter(X[0:N1,-1], X[0:N1,-2], color='b', label='class +1')\n",
    "plt.scatter(X[N1:,-1], X[N1:,-2], color='red', label='class -1')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()\n",
    "plt.savefig(\"2dscatter-K=2classes.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 We help you out by providing a nice display function\n",
    "\n",
    "It plots the data AND the hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(X,T, wparameters, iteration):\n",
    "    plt.figure(1)\n",
    "    W1 = X[T==-1,1:] # points of class \"-1\"\n",
    "    W2 = X[T== 1,1:] # points of class \"+1\"\n",
    "\n",
    "    ## X is assumed to be an extended vector, so 3D for 2D data,\n",
    "    ## and wparameters must have as many parameters as D+1 (3 here)\n",
    "    assert(X.shape[1] == wparameters.shape[0])\n",
    "    \n",
    "    cmap = cm.jet\n",
    "    gradient=cmap(np.linspace(0.0,1.0,12))\n",
    "    w0 = wparameters[0]\n",
    "    A12 = wparameters[1:3]\n",
    "    # length of the normal vector (defining the plane)\n",
    "    norm = np.sqrt((A12**2).sum())\n",
    "    # computes the closest point to the origin (in the separating plane)\n",
    "    H = - w0 * A12 / (norm * norm)\n",
    "    # vector along the separating line / found by rotation / rotation performed by matrix multiplication\n",
    "    U = np.dot( np.array([[0, 1], [-1, 0]]) , A12) / norm\n",
    "    # segment around H, of length 10\n",
    "    D = np.vstack((H + 5*U, H - 5*U))\n",
    "\n",
    "#     plt.axis([-2, 5, -2, 5])\n",
    "    plt.plot(W1[:,0], W1[:,1], 'r+') # points of class \"-1\"\n",
    "    plt.plot(W2[:,0], W2[:,1], 'bx') # points of class \"+1\"\n",
    "    plt.plot(D[:,0], D[:,1], ls='-', color=gradient[iteration%(len(gradient))])\n",
    "    # ax.set_aspect(1) # to set an aspect ratio of 1, to have a squared box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wparameters = np.array([1,0.01,1])\n",
    "display(X,T, wparameters, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization\n",
    "\n",
    "- make a stupid or random guess for an initial value of the parameters vector `wparameters`, i.e set a value for $\\vec w_0=$ `w0`, the initial value of $\\vec w$\n",
    "- choose some values for the hyper-parameters `eta`, `MaxIter`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of weights\n",
    "\n",
    "It's independent of the rest, we just need to know the dimensionality of parameters to be initalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeWeights(X, type):\n",
    "    ## data dimension\n",
    "    D = X.shape[1]\n",
    "\n",
    "    ## choice 1: deterministic\n",
    "    if type==\"zero\":\n",
    "        wparameters = np.zeros((D))\n",
    "        wparameters[0] = 0.00001\n",
    "\n",
    "    ## choice 2: uniform random centered in 0\n",
    "    ##           with appropriate order of magnitude\n",
    "    elif type==\"random\":\n",
    "        wparameters = ???\n",
    "\n",
    "    return wparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of points is obtained dynamically\n",
    "N = X.shape[0]\n",
    "\n",
    "######### initialization of the separating vector ######\n",
    "w0 = initializeWeights(X,\"zero\")\n",
    "w0[0]=1\n",
    "\n",
    "## random ##\n",
    "np.random.seed(42)\n",
    "w0 = initializeWeights(X,\"random\")\n",
    "\n",
    "display(X,T, w0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyper-parameters ###\n",
    "MAXITER = ??\n",
    "eta = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Update steps, pen-and-paper\n",
    "\n",
    "You should have done exercise \"Perceptron – classics\" by now, so **this should already be done**\n",
    "\n",
    "Otherwise, **take a piece of paper and a pencil, and derive the update rule for the perceptron**, according to the method of Gradient Descent. Really do it on paper, not on the computer !! \n",
    "\n",
    "Note: we start with the perceptron using the *full batch* strategy. Variants include the *Online* strategy, or the *SGD* strategy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Update steps, coding it \n",
    "\n",
    "- implement a function perceptronFullBatch(X,T,eta, w0, maxIter=20, plot=True)\n",
    "- run it !\n",
    "- add a print of the number of errors, so as to keep track of train and test error at all iterations of the algorithm (here for now we did not define a validation set, nor a test set)\n",
    "\n",
    "If you have a hard time with python slices (boolean filters), you can first implement the stupid J2 Loss function (see slides), which has a simpler implementaiton code, because we update for all examples, whether they are corrrectly classified or not.\n",
    "\n",
    "Test your code on the toy data set, and display your parameters (i.e. display the hyperplane, since we are in 2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, wparameters.shape, T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit of trial-and-error on shapes:\n",
    "(((X@ wparameters))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptronFullBatch(X,T,eta, w0, maxIter):\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    wparameters = w0.copy()  # deep copy\n",
    "\n",
    "    ## the algo itself ##\n",
    "    for iteration in range(maxIter):\n",
    "        \n",
    "        ## TODO\n",
    "        ??\n",
    "        \n",
    "    return wparameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wparameters =  perceptronFullBatch(X,T,eta, w0, MAXITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Testing on more realistic data: MNIST (restricted to 2 classes)\n",
    "\n",
    "Test your code on the following data set. You should be able to fully fit the train set.\n",
    "\n",
    "Question: can you still picture the hyperplane ? Remember that we have many dimensions, but that the data consists in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MNIST, reduced in quality (8x8 pixels pictures) ##\n",
    "K=2\n",
    "import sklearn.datasets\n",
    "Xmnist,Tmnist = sklearn.datasets.load_digits(n_class=K, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xmnist.shape, Tmnist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We show a couple of pictures and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10):\n",
    "    plt.figure()\n",
    "    plt.imshow(Xmnist[n].reshape(8, 8) , cm.gray)\n",
    "    print(Tmnist[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the labels are encoded as 0, 1 and not -1, 1. Is that a problem ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tmnist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : do something about it !?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tmnist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w0mnist = initializeWeights(Xmnist,\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta=0.01\n",
    "MAXITER=100\n",
    "wparameters =  perceptronFullBatch(Xmnist,Tmnist,eta, w0mnist, MAXITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wparameters.reshape(8, 8) , cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Bonus / Homework: Re-code your algo, putting it inside a class\n",
    "\n",
    "The class should contain an `__init__` method, a `fit` method and a `predict` method, at least. It may also include a `score` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bonus / Homework: Compare the evolution of the hyperplane in the Online and Full-Batch strategies, on the following data set: \n",
    "- for this data set (and maybe also the one at the beggining of the TP), get an intuitive feeling of how the plane evolves over iterations, for the online perceptron and for the full batch perceptron\n",
    "- which algo seems the most efficient ? (Try several initializations of $\\vec w_0$).\n",
    "\n",
    "#### Note: \n",
    "this is the data set used in exercise \"2.1.2 Roseblatt’s online Perceptron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data set 1\n",
    "Xtoy = np.array([ [0, 0], [0, 1], [1, 0], [1, 1], [4, 0], [4, 1] ])\n",
    "Ttoy = np.array([ -1 , -1 , 1 , 1 , 1 ,1])\n",
    "N = Xtoy.shape[0]\n",
    "Xtoy =  np.hstack((np.ones((N,1)), Xtoy))  # extended vector\n",
    "wparameters_toy = np.array([1, 0.00, 1])\n",
    "display(Xtoy,Ttoy, wparameters_toy, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting = True\n",
    "eta=0.2\n",
    "MAXITER=30\n",
    "wparameters =  perceptronFullBatch(Xtoy,Ttoy,eta, wparameters_toy, MAXITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptronOnline(X,y,eta, w0, maxIter=20, plot=True):\n",
    "    # sets the number of points\n",
    "    N = X.shape[0]\n",
    "\n",
    "    wparameters=w0.copy()   # deep copy\n",
    "    ## the algo itself\n",
    "    for iteration in range(maxIter):\n",
    "    \n",
    "        ## TODO\n",
    "        ??\n",
    "        \n",
    "    return wparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting = True\n",
    "eta=0.2\n",
    "MAXITER=30\n",
    "wparameters =  perceptronOnline(Xtoy,Ttoy,eta, wparameters_toy, MAXITER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Bonus / Homework: use sklearn to check your codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn ## this line should run, at home (check it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
