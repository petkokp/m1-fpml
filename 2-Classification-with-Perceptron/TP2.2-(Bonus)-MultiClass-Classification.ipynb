{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Multi class classification\n",
    "\n",
    "\n",
    "The multi-class percpetron can be implemented this way. \n",
    "We denote $K$ the number of classes, $N$ the number of (training) examples, $D$ the dimension of the data (after feature augmentation, at least with a \"1\" as first component).\n",
    "\n",
    "The **output** of the network *(not equal to the predicted label)*, can be taken as the **softmax** among the $K$ separating hyperplanes (each hyperplane $\\vec{w}_k$ separates class $k$ from the others).\n",
    "$$ y_k^{(n)} = \\text{softmax}\\big( (\\vec{w}_{k} \\cdot \\vec{x}^{(n)})_{k=1...K} \\big) = \\frac{ \\exp(  \\vec{w}_k\\cdot\\vec{x}^{(n)}   )}{\\sum_\\ell \\exp(  \\vec{w}_\\ell\\cdot\\vec{x}^{(n)})}$$\n",
    "This output can be **interpreted as the probability** that example $x^{(n)}$ belongs to the class $k$, according the classifier's current parameters\n",
    "Indeed, one can easily check that for any $\\vec{x}$, the sum of probabilities is indeed one : $\\sum_k y_k = 1$.\n",
    "The **total output of the network** is a vector $\\vec{y}^{(n)} = \\begin{pmatrix}y_1^{(n)} \\\\ y_2^{(n)} \\\\ .. \\\\ y_K^{(n)} \\end{pmatrix}$ (for the sample number $n$).\n",
    "\n",
    "The **true labels (ground truth)** of example $\\vec{x}^{(n)}$ is then encoded as a one-hot vector, so that if the example is of the second class, it may be written: $\\vec{t}^{n} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ .. \\\\ 0 \\end{pmatrix}$. (where $\\vec{t}^{(n)}$ or $\\vec{t}^{n}$ is for **T**ruth and is shorter to write than $\\vec{y}^{GT,(n)}$). More generally, the components $t_{n,k}$ of vector $\\vec{t}_n$ may be written using the Kronecker's delta: $t_{n,k} = \\delta(k, k_{true}^n)$, where $k_{true}^n$ is the true class of example number $n$.\n",
    "\n",
    "From now on, **we drop the superscrip $a^{(n)}$ and instead write $a_n$ or just $a$**, when it's clear enough that the quantity $a$ relates to a single example, of generic index $n$. This helps to lighten the notations.\n",
    "\n",
    "The Loss function that we should use is called the **cross-entropy loss function**, and is:\n",
    "\n",
    "$$J = \\frac1N \\sum_n^N H(\\vec{t}_{n}, \\vec{y}_{n})$$\n",
    "\n",
    "where the cross-entropy is a non-symmetric function: $$H(\\vec{t}_{n}, \\vec{y}_{n}) = -\\sum_k^K t_{n,k} \\log (y_{n,k})$$ \n",
    "\n",
    "Make sure you undersand all of the above. Write down the Loss function for the multi-class perceptron. \n",
    "### Part 2.1\n",
    "- What are the parameters of the model ? **How many real numbers is that ?** Count them in terms of $N,K,D, etc$. \n",
    "- (3-4 points) **Derive the update steps for the gradient**. (you can get inspiration from TD4.1)\n",
    "- Some Hints:\n",
    "    - It is recommended to compute the quantity $\\nabla_{w_\\ell} y_k$ ($\\ell\\neq k$) and the quantity $\\nabla_{w_k} y_k$. Try to express these simply, by recognizing $y$ when it appears. First treat the two cases separately, then try to unite the two cases in a single mathematical form, using Kronecker's delta : $\\delta(i,j)= \\{1$ if $i=j$, else $0\\}$.\n",
    "    - When there is a sum $\\sum_\\ell f(w_\\ell)$ and you derive with respect to $w_k$, the output only depends on the term $f(w_k)$ \n",
    "    - In the sum above, $\\sum_\\ell f(w_\\ell)$ the index $\\ell$ is a \"mute\" index: you can use any letter for it. Be careful not to use a letter that already exists outside the sum ($\\ell$ is like a local variable, don't use the same name for a \"global variable\" from outside the function !)\n",
    "    - For any functions $u,v$ that admit derivatives, $\\partial_x \\frac{u(x)}{v(x)} = \\frac{u'(x)v(x)-u(x)v'(x)}{(v(x))^2}$. It extends to $\\nabla_x$ without problem.\n",
    "    - $\\nabla_x \\exp(u(x)) =  \\exp(u(x)) \\nabla_x u(x)$.\n",
    "    - $\\frac{a}{1+a} = 1- \\frac{1}{1+a}$\n",
    "    - $\\partial_x \\log(u(x)) = \\frac{u'(x)}{u(x)}$ \n",
    "    - If you are too much blocked, you can ask me (via discord, in Private Message) for the solution of $\\nabla_{w_k} y_k$ and/or the solution for $\\nabla_{w_\\ell} y_k$ ($\\ell\\neq k$).\n",
    "    - In the end, the update step for the parameters that you should find is : $$ \\vec{w}_\\ell \\mapsto \\vec{w}_\\ell + \\eta \\frac1N \\sum_n^N \\vec{x}_n (\\delta_{\\ell, k_{true}^n}- y_{\\ell,n})$$\n",
    "    - If you cannot find the equation above, you can just skip this question and use it to make your program.\n",
    "    \n",
    "    \n",
    "\n",
    "### Part 2.2\n",
    "- (3 points) **Think up of all the functions you need to write**, and **put them in a class** (you can get inspiration from the correction of TP3.2) - first write a class skeleton, and **only then, write the methods** inside\n",
    "- Hints:\n",
    "    - there may be numerical errors (NaNs) because $\\exp(..)$ is too large. You can ease this by noticing the following: for any positive constant $C$, we have $$\\frac{ \\exp( a_k  )}{\\sum_\\ell \\exp (a_\\ell) }  = \\frac{C \\exp( a_k  )}{C \\sum_\\ell \\exp (a_\\ell) }= \\frac{\\exp( a_k +\\log C )}{\\sum_\\ell \\exp (a_\\ell +\\log C) }$$\n",
    "    - with this trick, when your arguments in the softmax are too large, you can simply subtract a big constant $\\log C$ from its argument, and this will reduce the chances of numerical error, without changing the result. It's a good idea to change the $w $'s with this kind of trick.\n",
    "    - it's a good idea to define the target labels (ground truth) data in one-hot vectors (as said above), compute them once and for all, and then you never have to compute them again. In practice, you may notice that for an example with label $k_{true}$, then the genreic component number of $k$ of the vector $\\vec{t}$ reads: $t_{k} = \\delta_{k, k_{true}}$\n",
    "    - the initial $w$ should be random (not all zeros), preferably, but not too big. A good idea is to have their dispersion be of order $1/D$ at most.\n",
    "    \n",
    "For this question, the main goal is to make a theoretically-working, rather clean code, using numpy array-operations (`np.dot`) and not loops, as much as possible. If you manage to do that, you will most likely have a working code (and fast code!)\n",
    "- (1 point) Test your algorithm on Fashion-MNIST: make a train / validation / test split , fit the model, compute the cross-val error, and the test error. Don't waste time on optimizing hyper-parameters (just take an $\\eta$ small enough that you kind of converge. The goal is really to prove that your algorithm does not always crash :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6881171418161356e+43\n"
     ]
    }
   ],
   "source": [
    "## remark:\n",
    "import numpy as np\n",
    "print(np.exp(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flandes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/flandes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(800)- np.exp(800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
