{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using Gradient Descent\n",
    "\n",
    "We want to solve linear regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark about the data**: here, we work with a **toy example** where the data is generated from a linear relationship + some noise. So, we can actually read the true value of the coefficients. \n",
    "\n",
    "But of course, **in real life, you do not have access** to the true model which generated the data, and certainly not access to its parameters either !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We help you create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createData(N,theta0,theta1,sigmaNoise): \n",
    "    Xrange = 10\n",
    "    D = 1\n",
    "    ## x are random\n",
    "    X = np.random.random((N,D))*Xrange   \n",
    "    ## the \"y\" (ordinates axis) are y=a.x+b + noise\n",
    "    noise = np.random.normal(0,sigmaNoise, (N,D))\n",
    "    y = theta0 + theta1*X + noise   # not available in real life\n",
    "\n",
    "    # for your numpy education, here is another method to create the data:\n",
    "    theta = np.array([theta0, theta1]).reshape((2,1))\n",
    "    Xaugmented = np.hstack((np.ones((N,1)) , X )) # this 1 is really a 1, not a D !\n",
    "    y2 = (Xaugmented @ theta) + noise\n",
    "    \n",
    "    print(\"difference in the 2 methods:\",  (abs(y-y2)).sum()  ) # we then check that this prints ~0\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### data creation #########\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "N=100\n",
    "theta0=1.1                                                         # not available in real life\n",
    "theta1=0.5                                                         # not available in real life\n",
    "sigmaNoise = 0.5                                                   # not available in real life\n",
    "X, y = createData(N,theta0,theta1,sigmaNoise) ## training data \n",
    "Ntest=N//2\n",
    "Xtest, ytest = createData(Ntest,theta0,theta1,sigmaNoise) ## test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### data visualization ###########\n",
    "plt.figure()\n",
    "plt.scatter(X,y, label=\"train\", marker='x',color='k')\n",
    "plt.scatter(Xtest,ytest, label= \"test\", marker='x',color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: handcrafted model\n",
    "\n",
    "## 1. Model defintion, initalization\n",
    "\n",
    "- create a model, i.e. a function $f_\\Theta(x)$\n",
    "- choose hyper-parameters `eta`, `MaxIter`, and set the dimension of data `D` appropriately (if you have no idea, put a stupid thing to start, you'll make it better later. Ask around, also ! This is not very important for now).\n",
    "- make a stupid or random guess for an initial value of the parameters vector `theta`, i.e. for `thetaInitial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the model ###\n",
    "def model(X, theta):\n",
    "## it's only 1-2 lines..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### hyper-parameters #####\n",
    "eta = ??\n",
    "MaxIter = ??\n",
    "D = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialization (in a sense, a hyper-parameter)\n",
    "thetaInitial = np.array([??])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark about the auxiliary ones trick:\n",
    "\n",
    "If you prefer to write things using the trick of augmented X, to use more matrix operations, \n",
    "- you're right !\n",
    "- go see the d-dimensional case, where the data X is defined with its augmented component (filled with ones)\n",
    "\n",
    "But if you prefer not to, for now, it's ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Update rule\n",
    "\n",
    "### Now the crucial part \n",
    "\n",
    "- **take a piece of paper and a pencil, and derive the update rule for your function $f_\\Theta(x)$**, according to the method of Grandient Descent. Really do it on paper, not on the computer !!\n",
    "Use the Mean Squared Error Loss.\n",
    "\n",
    "### Now to code it\n",
    "\n",
    "- implement a function `fit(model, eta, MaxIter, D, theta, X, y)` that will update the parameters according to gradient descent\n",
    "- run it !\n",
    "- add a recording of train and test error to your fit function, so as to keep track of train and test error at all iterations of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, eta, MaxIter, D, theta, X, y):\n",
    "    ## note : D is actually useless !\n",
    "    ??\n",
    "    for iteration in range(MaxIter):\n",
    "        ??\n",
    "    return ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the fit function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the model, testing the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow the results \n",
    "\n",
    "- plot the data (train and test), the initial guess, and the final model (all 4 things on a single graph).\n",
    "- on a separate plot, show the *training error* and *test errror* evolution over the iterations (epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the data, the noiseless-teacher, the model that was learned\n",
    "plt.figure(1)\n",
    "plt.scatter(X,y, label=\"train\", marker='x',color='k')\n",
    "plt.scatter(Xtest,ytest, label= \"test\", marker='x',color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## follow the evolution of train and test accuracy or error\n",
    "epochs = np.arange(MaxIter)\n",
    "plt.figure(2)\n",
    "plt.loglog(epochs, ??, label='train error')\n",
    "plt.loglog(epochs, ??, label='test error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: sklearn solution\n",
    "\n",
    "## Comparing with the sklearn implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now cheat using `sklearn`, to check wether our results are consistent with it (they will often be *slightly*  different!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check these lines at home   ##\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg_sklearn_object = sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
    "Xreshaped = X.reshape((N,1))\n",
    "reg_sklearn_object.fit(Xreshaped,y)\n",
    "print(\"coeffs fitted by sklearn: \", reg_sklearn_object.coef_, reg_sklearn_object.intercept_)\n",
    "\n",
    "Xtest_resh = Xtest.reshape((Ntest,1))\n",
    "print(reg_sklearn_object.score(Xreshaped, y))\n",
    "print(reg_sklearn_object.score(Xtest_resh, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_sklearn = np.array([reg_sklearn_object.intercept_, reg_sklearn_object.coef_[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the data, the noiseless-teacher, the model that was learned\n",
    "plt.figure(1)\n",
    "plt.scatter(X,y, label=\"train\", marker='x',color='k')\n",
    "plt.scatter(Xtest,ytest, label= \"test\", marker='x',color='red')\n",
    "plt.plot(X, model(theta_sklearn,X), color='teal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: read and understand well each line in the sklearn piece of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3:  the augmented X trick:\n",
    "\n",
    "### Warning:\n",
    "\n",
    "Make sure to complete the pen-and-paper exercise 1.1.2 of \"Regressions.pdf\" before trying to do this !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add ones where we need them\n",
    "Xa        = np.hstack((np.ones((N,1))    , X))\n",
    "Xtest_aug = np.hstack((np.ones((Ntest,1)), Xtest))\n",
    "theta_aug = thetaInitial.reshape(D+1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(X, theta):\n",
    "    return X@theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_augmented(model, eta, MaxIter, D, theta, X, y, Xtest, ytest):\n",
    "    \n",
    "    ???\n",
    "    \n",
    "    return theta, trainError, testError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, trainError, testError = fit_augmented(model2, eta, MaxIter, D, theta_aug, Xa, y, Xtest_aug, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------- \n",
    "# (optional) $d$-dimensional input\n",
    "\n",
    "### This is Optional Homework\n",
    "\n",
    "-> See TP1.2 for some help to start on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------- \n",
    "# (optional) Polynomial regression\n",
    "\n",
    "### Optional Homework: do the same, but using a 3-rd order polynomial (in data generation, and in your model) \n",
    "\n",
    "(advice: keep a $D=1$ input)\n",
    "\n",
    "-> See TP1.3 for some help to start on this\n",
    "\n",
    "-> We'll see a smarter trick to do that (aka Kernels/feature maps) later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
