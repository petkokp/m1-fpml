{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.cm as cm\n",
    "import sklearn\n",
    "\n",
    "import TP3_helper_function_dont_look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### uncomment the line below to get figures in pop-up windows, that you can then drag and turn around \n",
    "### (nice to see 3D plots correctly)\n",
    "# %matplotlib qt  \n",
    "### disable the line above if you have errors, or if you prefer figures to remain \n",
    "### embedded in the notebook (no pop-ups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y):\n",
    "    plt.figure(figsize=[5,5]) ## equal x and y lengths for a squared figure\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s = 100)\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    #plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, yregress, yclassif = TP3_helper_function_dont_look.getData(42)\n",
    "X_test, yregress_test, yclassif_test = TP3_helper_function_dont_look.getData(41, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "In this TP, we sometimes use a very large amount of test data, so as to get a \"true value\" (not really \"true\", but quite converged) for the test error. This is useful pedagogically, to understand how $N_{train}$ or hyper-parameters can impact the quality of the model that is learned. \n",
    "\n",
    "In real life, of course, you generally use more data for training than for testing, since it's better to improve the results (increase $N_{train}$) than to improve the accuracy of measurement of the (test) error (increase $N_{test}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: classification of the XOR data set\n",
    "\n",
    "You're going to code your own feature map, so as to classify the XOR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X, yclassif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the default feature map is the identity function\n",
    "def defaultFeatureMap(X):\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a function to plot the domains of prediction (for a classification)\n",
    "## the idea is to grid the (2D) pre-feature-map space with a mesh, and \n",
    "## display the predicted class with a color, in each little square of the mesh.\n",
    "def plot_boundary(clf, X, y, featureMap=None):\n",
    "    if featureMap == None:\n",
    "        featureMap = defaultFeatureMap\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1 \n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    hx = hy = 0.002 ## grid mesh size\n",
    "    hx = (x_max-x_min)/200 ## grid mesh size\n",
    "    hy = (y_max-y_min)/200 ## grid mesh size\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, hx),\n",
    "                         np.arange(y_min, y_max, hy))\n",
    "    Z = clf.predict(featureMap(np.c_[xx.ravel(), yy.ravel()])) ## prediction value by zone\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=[5,5]) ## equal x and y lengths for a squared figure\n",
    "    plt.title('score : ' + str(clf.score(featureMap(X),y)))    \n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s = 10)\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder: full-batch Perceptron (linear classifier)\n",
    "\n",
    "Or actually, we may simply call it linear classifier.\n",
    "\n",
    "## Question 1.1 : Complete the class that is provided below\n",
    "\n",
    "Then run it on your data, `X`, `yclassif`\n",
    "\n",
    "### About python classes\n",
    "\n",
    "- members of the class are accessed with the syntax `self.myMemberObject` (regardless of its nature, function, variable, sub-class, etc).\n",
    "- functions (methods) of the class always take the argument `self` as first argument. Look at the example of the two lines of code, `def initializeWeights(self,D):` and  `w = self.initializeWeights(D)`. You see that `self` does not need to be passed as an argument because it's already present when we do `self.MyFunction`\n",
    "- the `__init__` function initializes (instanciates) an instance of the class with some parameters (default values or passed as arguments of the constructor when a instance is created)\n",
    "\n",
    "You can also check out what is expected from a typical sklearn class by looking at https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression or  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classLinearClassifierFullBatch():\n",
    "\n",
    "    def __init__(self, eta=0.001, maxIter=100, seed=42, verbose=True, fit_intercept=True):\n",
    "        self.eta = eta\n",
    "        self.maxIter = maxIter\n",
    "        self.seed = seed\n",
    "        self.w = None # at the start, it's undefined\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def initializeWeights(self,D):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, Xraw, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self,Xraw):\n",
    "        return ??\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return ??\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the order of the parameters does not matter (since they have keywords)\n",
    "clf = classLinearClassifierFullBatch(eta=0.01, seed=41, maxIter=3000) \n",
    "clf.fit(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(clf, X, yclassif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2 : \n",
    "\n",
    "Are you happy with this classification ?\n",
    "\n",
    "What can be done to improve it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3 : make your own feature map ! \n",
    "\n",
    "- Define a feature map. For instance, you may use a polynomial feature map. Go back to the lecture notes if you are out of ideas. Simpler is better ! (at least for today)\n",
    "- create a new vector $X_f = \\phi(X)$, i.e. the transform of your dataset through this feature-map\n",
    "- use it as input in our LINEAR classification model\n",
    "- look at the score and plot the result using plot_boundary()\n",
    "\n",
    "Advice: create a new instance of your classifier class, so as to not confuse \n",
    "- the fitted model which expects the raw data and \n",
    "- the fitted model which expects the augmented (featurized) data\n",
    "\n",
    "If we did things right, we do not need to change our model (the whole python class) AT ALL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMap(X):\n",
    "    return ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: check carefully that your data after applying the feature map has the reasonable shape. If not, transpose it, or do something with your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureMap(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = classLinearClassifierFullBatch(eta=0.003, seed=41, maxIter=30000) # order of parameters does not matter\n",
    "??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_boundary(clf2, X, yclassif, featureMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.4 : are you happy with this classification ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.5: compute the test error/score,and display the results for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(clf2, X_test, yclassif_test, featureMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.6:\n",
    "\n",
    "build the *learning curve* of the problem.\n",
    "\n",
    "You may choose an exponentially growing number of training examples, such as `Ntrains = [10,30,100,300,1000, 3000, 10000]`, or `Ntrains = [2**k for k in range(10)]` and a large number of test examples, for the sake of having a precise estimation of the test error. You should probably use log-log or semilog- plots.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrains = [2**k for k in range(10)]\n",
    "\n",
    "clf2 = classLinearClassifierFullBatch(eta=0.01, seed=41, maxIter=30000, verbose=False) # order of parameters does not matter\n",
    "score_train =[]\n",
    "score_test  =[]\n",
    "??\n",
    "for Ntrain in Ntrains:\n",
    "    X, yregress, yclassif = TP3_helper_function_dont_look.getData(42, Ntrain)\n",
    "    ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.semilogx(??)\n",
    "\n",
    "plt.figure()\n",
    "plt.loglog(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: same thing but with regression !\n",
    "\n",
    "Now we re-do the same thing but for a regression task.\n",
    "The data is `X, yregress`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import TP3_helper_function_dont_look\n",
    "X, yregress, yclassif = TP3_helper_function_dont_look.getData(42)\n",
    "X_test, yregress_test, yclassif_test = TP3_helper_function_dont_look.getData(41, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[:5,:], yregress[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twod_scatter_plot_colored_value(X, y):\n",
    "    plt.scatter(X[:,0], X[:,1], s=10, c=y, cmap='jet')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twod_scatter_plot_colored_value(X, yregress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## another way to plot, less legible in my opinion\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "#Labeling\n",
    "ax.set_xlabel('X Axes')\n",
    "ax.set_ylabel('Y Axes')\n",
    "ax.set_zlabel('Z Axes')\n",
    "ax.plot3D(X[:,0], X[:,1], yregress, ls='', marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 : code your regressor !\n",
    "\n",
    "define a class `classLinearRegressorFullBatch`  that will perform regression, in a similar fashion as `classLinearClassifierFullBatch` did perform a (binary) classification.\n",
    "There should be only 2,3 or 4 lines at most to change.\n",
    "\n",
    "Remember:\n",
    "- the model and cost function are essentially (or exactly?) the same\n",
    "- the prediction now takes in values in $\\mathbb{R}$\n",
    "- the score is now defined not as the number of correctly classified points, but as the Mean Squared Error. In other terms, it's essentially equal to the Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_regress(reg, X, y, featureMap=None):  \n",
    "    if featureMap == None:\n",
    "        featureMap = defaultFeatureMap\n",
    "    h = 0.02 ## grid mesh size\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1 \n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    Z = reg.predict(featureMap(np.c_[xx.ravel(), yy.ravel()])) ## prediction value by zone\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx,yy,Z)\n",
    "    plt.colorbar()\n",
    "    twod_scatter_plot_colored_value(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#the other kind of plot \n",
    "def plot_prediction_regress_2(reg, X, y, featureMap=None):  \n",
    "    if featureMap == None:\n",
    "        featureMap = defaultFeatureMap\n",
    "    h = 0.02 ## grid mesh size\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1 \n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection=\"3d\")\n",
    "    Z = reg.predict(featureMap(np.c_[xx.ravel(), yy.ravel()])) ## prediction value by zone\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.title('score : ' + str(reg.score(featureMap(X),y)))  \n",
    "    ax.plot3D(xx.flatten(), yy.flatten(), Z.flatten(),marker='o', ls='',color=\"green\")\n",
    "    ax.scatter(X[:, 0], X[:, 1], y, c=y, s = 100)\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classLinearRegressorFullBatch():\n",
    "\n",
    "    def __init__(self, eta=0.001, maxIter=100, seed=42, verbose=True, fit_intercept=True):\n",
    "        self.eta = eta\n",
    "        self.maxIter = maxIter\n",
    "        self.seed = seed\n",
    "        self.w = None # at the start, it's undefined\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def initializeWeights(self,D):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, Xraw, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self,Xraw):\n",
    "        return ??\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return ??\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg1 = classLinearRegressorFullBatch(eta=0.01, seed=42, maxIter=3000) # order of parameters does not matter\n",
    "??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_prediction_regress(reg1, X, yregress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2: does it work well ? why ?\n",
    "\n",
    "## Answer: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3\n",
    "\n",
    "- As before, use a trick to make your LINEAR algorithm become really good.\n",
    "- plot the predictions and data using the function `plot_prediction_regress()`\n",
    "- are you happy now ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg2 = classLinearRegressorFullBatch(eta=0.01, seed=42, maxIter=30000) # order of parameters does not matter\n",
    "??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_prediction_regress(reg2, X, yregress, featureMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quesiton 2.4\n",
    "\n",
    "Compute also the test error and plot the prediction on the test data\n",
    "\n",
    "(in this case it's not very instructive, but it's a good habit to take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_prediction_regress(reg2, X_test, yregress_test, featureMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.5:\n",
    "\n",
    "build the *learning curve* of the problem.\n",
    "\n",
    "You may choose an exponentially growing number of training examples, such as `Ntrains = [2**k for k in range(10)]`, and a large number of test examples, for the sake of having a precise estimation of the test error. You should probably use log-log or semilog- plots.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Ntrains = [2**k for k in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "If you run the code below (**Part 3**) and want to play again with your code above this point, you should re-load the part 1&2 data !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, yregress, yclassif = TP3_helper_function_dont_look.getData(42)\n",
    "X_test, yregress_test, yclassif_test = TP3_helper_function_dont_look.getData(41, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : the moon data set\n",
    "\n",
    "The so-called moons data set can be generated with sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "data = make_moons(noise = 0.1, random_state=1, n_samples=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1: solve the task\n",
    "\n",
    "- On your own, identify which kind of task is at hand:\n",
    "    - what kind of data is it ?\n",
    "    - is it supervised, unsupervised ? Which sub-class of ML is it ?\n",
    "    - is the data in the form you need (label values taking the expected kind of values for instance) ? Is it well standardized ?\n",
    "- Using a simple (linear) model that you already have from previous work, try to solve the task.\n",
    "- are you satisfied with the result ? What can we do ? \n",
    "- you may need to use a slightly more complicated feature map than before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to 3.1: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 : pen-and-paper exercise, to do at home\n",
    "\n",
    "This is to get a bit of practice with mathematics and understand the violence of polynomials\n",
    "\n",
    "- Take a piece of paper and compute $(1+\\vec{x}\\cdot\\vec{x}')^2$, but for $D=3$, and try to write down the corresponding $\\phi(\\vec{x})$. Do it also for $D=4$.\n",
    "- Take a piece of paper and compute $(1+\\vec{x}\\cdot\\vec{x}')^3$, but for $D=2$, and try to write down the corresponding $\\phi(\\vec{x})$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
