{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import sklearn\n",
    "plt.ion()\n",
    "\n",
    "import sklearn.svm\n",
    "import sklearn.tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We provide you 3 datasets to play with :\n",
    "- MNIST in small resolution (8x8 pixels images)\n",
    "- MNIST, classic: 28x28 resolution \n",
    "- Fashion_MNIST, 28x28 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data set 1 ##\n",
    "K=10\n",
    "import sklearn.datasets\n",
    "X,y = sklearn.datasets.load_digits(n_class=K, return_X_y=True)\n",
    "linearPictureLength = 8 # Global variable\n",
    "ratio_train = 0.6 # we may reduce this number when using mnist70 (70 000 images !)\n",
    "ratio_valid = 0.3 # same here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "## data set 2 ##\n",
    "LoadObject = np.load(\"../00-tools+datasets/mnist70.npz\")\n",
    "linearPictureLength = 28\n",
    "X = LoadObject['X']\n",
    "y = LoadObject['y']\n",
    "del LoadObject\n",
    "ratio_train = 0.005\n",
    "ratio_valid = 0.1\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "## data set 3 ##\n",
    "LoadObject = np.load(\"../00-tools+datasets/fashion-mnist-reshaped.npz\")\n",
    "linearPictureLength = 28\n",
    "X = LoadObject['train_images']\n",
    "y = LoadObject['train_labels']\n",
    "Xtest = LoadObject['test_images']\n",
    "ytest = LoadObject['test_labels']\n",
    "X     = np.array(X    , dtype=float) ## change the type, which is iniitially unsigned-int (uint)\n",
    "Xtest = np.array(Xtest, dtype=float)\n",
    "del LoadObject\n",
    "ratio_train = 0.05\n",
    "ratio_valid = 0.1\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,  41.,\n",
       "       188., 103.,  54.,  48.,  43.,  87., 168., 133.,  16.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,\n",
       "         0.,   0.,  49., 136., 219., 216., 228., 236., 255., 255., 255.,\n",
       "       255., 217., 215., 254., 231., 160.,  45.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  14., 176., 222., 224., 212.,\n",
       "       203., 198., 196., 200., 215., 204., 202., 201., 201., 201., 209.,\n",
       "       218., 224., 164.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0., 188., 219., 200., 198., 202., 198., 199., 199., 201., 196.,\n",
       "       198., 198., 200., 200., 200., 200., 201., 200., 225.,  41.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  51., 219., 199., 203., 203.,\n",
       "       212., 238., 248., 250., 245., 249., 246., 247., 252., 248., 235.,\n",
       "       207., 203., 203., 222., 140.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0., 116., 226., 206., 204., 207., 204., 101.,  75.,  47.,  73.,\n",
       "        48.,  50.,  45.,  51.,  63., 113., 222., 202., 206., 220., 224.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0., 200., 222., 209., 203.,\n",
       "       215., 200.,   0.,  70.,  98.,   0., 103.,  59.,  68.,  71.,  49.,\n",
       "         0., 219., 206., 214., 210., 250.,  38.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0., 247., 218., 212., 210., 215., 214.,   0., 254., 243.,\n",
       "       139., 255., 174., 251., 255., 205.,   0., 215., 217., 214., 208.,\n",
       "       220.,  95.,   0.,   0.,   0.,   0.,   0.,  45., 226., 214., 214.,\n",
       "       215., 224., 205.,   0.,  42.,  35.,  60.,  16.,  17.,  12.,  13.,\n",
       "        70.,   0., 189., 216., 212., 206., 212., 156.,   0.,   0.,   0.,\n",
       "         0.,   0., 164., 235., 214., 211., 220., 216., 201.,  52.,  71.,\n",
       "        89.,  94.,  83.,  78.,  70.,  76.,  92.,  87., 206., 207., 222.,\n",
       "       213., 219., 208.,   0.,   0.,   0.,   0.,   0., 106., 187., 223.,\n",
       "       237., 248., 211., 198., 252., 250., 248., 245., 248., 252., 253.,\n",
       "       250., 252., 239., 201., 212., 225., 215., 193., 113.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  17.,  54., 159., 222., 193., 208.,\n",
       "       192., 197., 200., 200., 200., 200., 201., 203., 195., 210., 165.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  47., 225., 192., 214., 203., 206., 204., 204., 205.,\n",
       "       206., 204., 212., 197., 218., 107.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   1.,   6.,   0.,  46., 212., 195.,\n",
       "       212., 202., 206., 205., 204., 205., 206., 204., 212., 200., 218.,\n",
       "        91.,   0.,   3.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   1.,   0.,  11., 197., 199., 205., 202., 205., 206., 204.,\n",
       "       205., 207., 204., 205., 205., 218.,  77.,   0.,   5.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,   0.,   2., 191.,\n",
       "       198., 201., 205., 206., 205., 205., 206., 209., 206., 199., 209.,\n",
       "       219.,  74.,   0.,   5.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   2.,   0.,   0., 188., 197., 200., 207., 207., 204.,\n",
       "       207., 207., 210., 208., 198., 207., 221.,  72.,   0.,   4.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   2.,   0.,   0.,\n",
       "       215., 198., 203., 206., 208., 205., 207., 207., 210., 208., 200.,\n",
       "       202., 222.,  75.,   0.,   4.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   1.,   0.,   0., 212., 198., 209., 206., 209.,\n",
       "       206., 208., 207., 211., 206., 205., 198., 221.,  80.,   0.,   3.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,\n",
       "         0., 204., 201., 205., 208., 207., 205., 211., 205., 210., 210.,\n",
       "       209., 195., 221.,  96.,   0.,   3.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   1.,   0.,   0., 202., 201., 205., 209.,\n",
       "       207., 205., 213., 206., 210., 209., 210., 194., 217., 105.,   0.,\n",
       "         2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,\n",
       "         0.,   0., 204., 204., 205., 208., 207., 205., 215., 207., 210.,\n",
       "       208., 211., 193., 213., 115.,   0.,   2.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 204., 207., 207.,\n",
       "       208., 206., 206., 215., 210., 210., 207., 212., 195., 210., 118.,\n",
       "         0.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         1.,   0.,   0., 198., 208., 208., 208., 204., 207., 212., 212.,\n",
       "       210., 207., 211., 196., 207., 121.,   0.,   1.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0., 198., 210.,\n",
       "       207., 208., 206., 209., 213., 212., 211., 207., 210., 197., 207.,\n",
       "       124.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0., 172., 210., 203., 201., 199., 204., 207.,\n",
       "       205., 204., 201., 205., 197., 206., 127.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 188.,\n",
       "       221., 214., 234., 236., 238., 244., 244., 244., 240., 243., 214.,\n",
       "       224., 162.,   0.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   1.,   0.,   0., 139., 146., 130., 135., 135., 137.,\n",
       "       125., 124., 125., 121., 119., 114., 130.,  76.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We load the data and split it into train, validation and test set.\n",
    "\n",
    "It's good to do this early, and remember to only use the train set for most operations.\n",
    "\n",
    "The validation set may be used to find the best hyper-parameters\n",
    "\n",
    "The test can be used.. only **once** ! Then it \"expires\", like old food is wasted after a couple of weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subSets(X, ratio_train, ratio_valid):\n",
    "    ratio_test = 1 - ratio_train - ratio_valid #\n",
    "    assert(ratio_test>0)\n",
    "    Ntot   = X.shape[0]\n",
    "    Ntrain = int(ratio_train*Ntot)\n",
    "    Nvalid = int(ratio_valid*Ntot)\n",
    "    Ntest  = Ntot - Ntrain - Nvalid\n",
    "    X_train = X[0: Ntrain].copy()\n",
    "    y_train = y[0: Ntrain].copy()\n",
    "    X_valid = X[Ntrain:Ntrain+Nvalid].copy() #  X[-Ntest:] also does the same\n",
    "    y_valid = y[Ntrain:Ntrain+Nvalid].copy() # \n",
    "    X_test  = X[-Ntest:].copy()\n",
    "    y_test  = y[-Ntest:].copy()\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## actually load the data into separate arrays\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = load_subSets(X, ratio_train, ratio_valid)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note  \n",
    "\n",
    "You can also use `sklearn.model_selection.train_test_split`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First part: getting a sense of what PCA does\n",
    "\n",
    "## 1.1 Normalization\n",
    "\n",
    "- take a quick look at the data: what does it look like ? What are the min and max values ?\n",
    "- normalize the input so all values of the training set are between 0 and 1. Perform the appropriate operation on the val and test sets so that they are consistent with this pre-processing operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   1.,   0.,   0.,  13.,  73.,   0.,   0.,   1.,   4.,   0.,\n",
       "          0.,   0.,   0.,   1.,   1.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   3.,   0.,  36., 136., 127.,  62.,  54.,   0.,   0.,   0.,\n",
       "          1.,   3.,   4.,   0.,   0.,   3.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   6.,   0., 102., 204., 176., 134., 144., 123.,  23.,   0.,\n",
       "          0.,   0.,   0.,  12.,  10.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0., 155., 236., 207., 178., 107., 156., 161., 109.,\n",
       "         64.,  23.,  77., 130.,  72.,  15.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          1.,   0.,  69., 207., 223., 218., 216., 216., 163., 127., 121.,\n",
       "        122., 146., 141.,  88., 172.,  66.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   1.,\n",
       "          1.,   0., 200., 232., 232., 233., 229., 223., 223., 215., 213.,\n",
       "        164., 127., 123., 196., 229.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0., 183., 225., 216., 223., 228., 235., 227., 224., 222.,\n",
       "        224., 221., 223., 245., 173.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0., 193., 228., 218., 213., 198., 180., 212., 210., 211.,\n",
       "        213., 223., 220., 243., 202.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   3.,\n",
       "          0.,  12., 219., 220., 212., 218., 192., 169., 227., 208., 218.,\n",
       "        224., 212., 226., 197., 209.,  52.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   6.,\n",
       "          0.,  99., 244., 222., 220., 218., 203., 198., 221., 215., 213.,\n",
       "        222., 220., 245., 119., 167.,  56.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   4.,   0.,\n",
       "          0.,  55., 236., 228., 230., 228., 240., 232., 213., 218., 223.,\n",
       "        234., 217., 217., 209.,  92.,   0.],\n",
       "       [  0.,   0.,   1.,   4.,   6.,   7.,   2.,   0.,   0.,   0.,   0.,\n",
       "          0., 237., 226., 217., 223., 222., 219., 222., 221., 216., 223.,\n",
       "        229., 215., 218., 255.,  77.,   0.],\n",
       "       [  0.,   3.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  62., 145.,\n",
       "        204., 228., 207., 213., 221., 218., 208., 211., 218., 224., 223.,\n",
       "        219., 215., 224., 244., 159.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,  18.,  44.,  82., 107., 189., 228., 220.,\n",
       "        222., 217., 226., 200., 205., 211., 230., 224., 234., 176., 188.,\n",
       "        250., 248., 233., 238., 215.,   0.],\n",
       "       [  0.,  57., 187., 208., 224., 221., 224., 208., 204., 214., 208.,\n",
       "        209., 200., 159., 245., 193., 206., 223., 255., 255., 221., 234.,\n",
       "        221., 211., 220., 232., 246.,   0.],\n",
       "       [  3., 202., 228., 224., 221., 211., 211., 214., 205., 205., 205.,\n",
       "        220., 240.,  80., 150., 255., 229., 221., 188., 154., 191., 210.,\n",
       "        204., 209., 222., 228., 225.,   0.],\n",
       "       [ 98., 233., 198., 210., 222., 229., 229., 234., 249., 220., 194.,\n",
       "        215., 217., 241.,  65.,  73., 106., 117., 168., 219., 221., 215.,\n",
       "        217., 223., 223., 224., 229.,  29.],\n",
       "       [ 75., 204., 212., 204., 193., 205., 211., 225., 216., 185., 197.,\n",
       "        206., 198., 213., 240., 195., 227., 245., 239., 223., 218., 212.,\n",
       "        209., 222., 220., 221., 230.,  67.],\n",
       "       [ 48., 203., 183., 194., 213., 197., 185., 190., 194., 192., 202.,\n",
       "        214., 219., 221., 220., 236., 225., 216., 199., 206., 186., 181.,\n",
       "        177., 172., 181., 205., 206., 115.],\n",
       "       [  0., 122., 219., 193., 179., 171., 183., 196., 204., 210., 213.,\n",
       "        207., 211., 210., 200., 196., 194., 191., 195., 191., 198., 192.,\n",
       "        176., 156., 167., 177., 210.,  92.],\n",
       "       [  0.,   0.,  74., 189., 212., 191., 175., 172., 175., 181., 185.,\n",
       "        188., 189., 188., 193., 198., 204., 209., 210., 210., 211., 188.,\n",
       "        188., 194., 192., 216., 170.,   0.],\n",
       "       [  2.,   0.,   0.,   0.,  66., 200., 222., 237., 239., 242., 246.,\n",
       "        243., 244., 221., 220., 193., 191., 179., 182., 182., 181., 176.,\n",
       "        166., 168.,  99.,  58.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  40.,  61.,  44.,  72.,\n",
       "         41.,  35.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[n].reshape(linearPictureLength, linearPictureLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=0\n",
    "plt.imshow(X_train[n].reshape(linearPictureLength, linearPictureLength) , cm.gray)\n",
    "print(y[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization: here we choose to simply perform min-max normalization\n",
    "\n",
    "What are the other choices ?\n",
    "\n",
    "- perform a simple normalization, setting the min vlaue to 0 and max value to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remark !\n",
    "\n",
    "We divide each sub-part of the data by the **same** value, not each by its max !\n",
    "\n",
    "This is important: in the spirit of pre-processing and train/val/test split, you must use only the training data to pre-process (all) the data, including the validation and test sets.\n",
    "\n",
    "The pipeline is not supposed to know in advance hte maximum value of the validation or test sets !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Your first PCA compression (+decompression)\n",
    "\n",
    "\n",
    "We want to see what typical PCA does on an image.\n",
    "- using the method `sklearn.decomposition.PCA` from the `scikit-learn` library, compress the training set (i.e. perform its PCA). Then, decompress this compressed version, and compare an image before and after its transformation.  For this, you will need to :\n",
    "    - define an instance of the class `sklearn.decomposition.PCA`, that we may call \"model\" or \"preProcessing\"\n",
    "    - use the methods `fit`, `transofrm` and `inverse_transform`\n",
    "    - use the plot funciton  `plot_before_vs_after_compression` that we provide below\n",
    "    - remember to make copies, not in-place transformation !\n",
    "    - to start, you can use an explained variance ratio of 95%\n",
    "- compare a couple of images (pick random images, just to take a look not just at 1 case)\n",
    "- compute the Squared Error (squared difference between original and compresse+decompressed images) for a single image.\n",
    "- compute the Mean Squared Error (same thing but averaged on all training set images).\n",
    "- by browsing the methods of `sklearn.decomposition.PCA`, find a way to display the *explained variance ratio*, for all possible values of the number of PCA components to be kept. Plot it on a graph.\n",
    "- display the before/after image comparison for a low number of components, e.g. 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_before_vs_after_compression(n, X_train, X_train_Transf_decompressed, y):\n",
    "    plt.figure()\n",
    "    plt.imshow(X_train[n].reshape(linearPictureLength, linearPictureLength) , cm.gray)\n",
    "    plt.title(\"this is supposed to look like a \"+str(y[n])+ \" before compression\")\n",
    "    plt.savefig(str(n)+\"_beforeCompression.png\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(X_train_Transf_decompressed[n].reshape(linearPictureLength, linearPictureLength) , cm.gray)\n",
    "    plt.title(\"this is supposed to look like a \"+str(y[n])+ \" after compression+decompression\")\n",
    "    plt.savefig(str(n)+\"_afterCompression.png\")\n",
    "\n",
    "    print(\"The MSE for this image is \", np.mean( (X_train[n]- X_train_Transf_decompressed[n])**2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1st attempt at PCA with an explained_variance ratio set to 0.95\n",
    "varianceExplained=0.95\n",
    "\n",
    "# TODO # \n",
    "\n",
    "## X_train_Transf_decompressed = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## displaying the total variance explained at various values of nComp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n=2\n",
    "plot_before_vs_after_compression(n, X_train, X_train_Transf_decompressed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varianceExplained=30 # 30 compnents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second part: make your own PCA !\n",
    "\n",
    "Now we want to really make sure we understand how PCA is, or can be done.\n",
    "\n",
    "## 2.1 Theory\n",
    "\n",
    "- Check out the lecture notes and make sure you understood the maths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 PCA \n",
    "\n",
    "Compute the PCA of the training set, using:\n",
    "- `np.mean(..., axis=...)` to compute the average of an array along one of its axes\n",
    "- `np.outer` computes the outer product of two vectors (you can also cheat and use `np.cov`... but it's cheating)\n",
    "- `.tranpose()` (applied on an array, or `np.transpose()` if you will). A shortcut for `a.transpose()` is `a.T`\n",
    "- `np.linalg.eig`, which computes the full system of eigenvalues and eigenvectors (with nornmalized eigenvectors)\n",
    "- `np.dot`, which compute the dot product (or matrix product). You can also use `np.matmul` if you have matrices.\n",
    "- `np.linalg.inv`, which computes the inverse of a matrix (and does not raise any warning when it is not invertibe, sadly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We check some identities to make sure we understand how `np.linal.eig` works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "a = np.random.random((3,3))\n",
    "w, v = np.linalg.eig(a)\n",
    "for i in range(a.shape[0]):\n",
    "    print(np.dot(a[:,:], v[:,i]) - w[i] * v[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(a, v) - np.dot(v, np.diag(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(np.abs(np.linalg.inv(v) - v.transpose() ))\n",
    "# ## eigenvectors are orthonrmal, i.e. their transpose and inverse is the same thing !\n",
    "# ## the tranpose is cost-less, compared to the inverse (very costly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.linalg.inv(v), np.dot(a, v)) - np.diag(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subtract the mean\n",
    "\n",
    "# TODO # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the covariance matrix\n",
    "\n",
    "# TODO # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## we can also cheat, with this: \n",
    "# np.cov(X_centered.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## diagonalize the covariance matrix\n",
    "\n",
    "# TODO # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose a number of components\n",
    "\n",
    "# TODO # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we keep only the first nComp eigenvectors (this is the compression part)\n",
    "\n",
    "# TODO # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we transform the whole dataset at once\n",
    "\n",
    "# TODO # \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Transform the compressed dataset back to the original space\n",
    "\n",
    "- remember that the \"decompression\" is done by $X_{back} = P^T . X'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO # \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4  compare with/without compression+decompression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third part: optimizing the number of components\n",
    "\n",
    "## 3. Optimization \n",
    "\n",
    "For this part, we consider the PCA not as an end in itself, but as a pre-processing tool.\n",
    "\n",
    "The idea is that if we have an over-parameterized model because the input dimension is too large, what we can do is compress it, and use the compressed \"image\" (which does not look like an image, since it has `nComp` dimensions..) as input. \n",
    "\n",
    "When `nComp` is too small, we will loose so much information about the input that the results will be worse. If `nComp` is too large, we do not compress much and so the model remains over-parameterized.\n",
    "\n",
    "**The goal of this part is to optimize the hyper-parameter `nComp` so as to get the best possible results.**\n",
    "\n",
    "For this, we are going to use an algorithm that we haven't yet seen in class: SVMs (*Support Vector Machines*), with a Polynomial *Kernel*, say of degree 3. We will see what this means in lecture 3.\n",
    "For now you can think of this as a black-box classification algorithms that simply \"does pretty good\".\n",
    "\n",
    "You are provided with these lines of code:\n",
    "\n",
    "`clf = sklearn.svm.SVC(C=0.01, kernel='poly', degree = 3, coef0 = 1)\n",
    "clf.fit(X_train_Transformed, y_train)`\n",
    "\n",
    "which define a SVM classfier with polynomial Kernel of degree 3 (and C is intentionally set a bit too low here).\n",
    "\n",
    "Here it is also advised to use the third data set (otherwise it's not very interesting), with not too much training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data set 3 ##\n",
    "LoadObject = np.load(\"../00-tools+datasets/fashion-mnist-reshaped.npz\")\n",
    "linearPictureLength = 28\n",
    "X = LoadObject['train_images']\n",
    "y = LoadObject['train_labels']\n",
    "Xtest = LoadObject['test_images']\n",
    "ytest = LoadObject['test_labels']\n",
    "X     = np.array(X    , dtype=float) ## change the type, which is iniitially unsigned-int (uint)\n",
    "Xtest = np.array(Xtest, dtype=float)\n",
    "del LoadObject\n",
    "ratio_train = 0.05\n",
    "ratio_valid = 0.1\n",
    "print(X.shape, X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test = load_subSets(X, ratio_train, ratio_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: \n",
    "\n",
    "You should use the function `clf.score` to compute the training score and validation score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.svm.SVC(C=0.01, kernel='poly', degree = 3, coef0 = 1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "## TODO: compute scores here\n",
    "\n",
    "print(\"no compression:    training score:\",trainscore, \". valid score:\", validscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nComp_range = np.arange(1,102,10)\n",
    "for nC in nComp_range:\n",
    "    \n",
    "    ## pre-processing \n",
    "    ## TODO\n",
    "\n",
    "    ## classification\n",
    "    ## TODO\n",
    "\n",
    "    ## measure of performance\n",
    "    ## TODO\n",
    "\n",
    "    ## recordings\n",
    "    ## TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot of scores vs the hyper-param nc (nothing to do here)\n",
    "plt.figure()\n",
    "plt.plot( ?? )\n",
    "plt.xlabel(\"nombre comp\")\n",
    "plt.ylabel(\"scores\")\n",
    "plt.legend()\n",
    "plt.ylim([0.5,1])\n",
    "\n",
    "## outlining where the best point is (validation set)\n",
    "## here we make a crude choice, which can be refined by hand, of course\n",
    "bestIndex = np.argmax( ??yourArrayHere?? )\n",
    "bestNC = nComp_range[bestIndex]\n",
    "plt.plot(bestNC, linear_valid_score[bestIndex], marker='X', color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Compute the test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
