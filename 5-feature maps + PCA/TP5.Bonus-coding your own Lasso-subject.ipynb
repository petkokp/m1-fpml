{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60245c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494eca1",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "we change a bit the input data, to get 2 dimensions for the input, to avoid troubles with matrix shapes (N,1) is different from (N,) in `python`, and this is often needlessly annoying.\n",
    "\n",
    "So dimension 0 is trivial, dimension 1 is interesting, and there is a **bias** (which will be handled by using `fit_intercept=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array([1,2,3,4])\n",
    "N=X.shape[0]\n",
    "X = np.vstack((np.zeros(N), X)).T  #or, we could do :  X.reshape((N,1))  ## then you need to change the code for plots\n",
    "wGT = np.array([0, 0.1])\n",
    "D=X.shape[1]-1\n",
    "y = X@wGT + 2\n",
    "y[1] -= 0.1\n",
    "y[2] += 0.1\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1],y)\n",
    "plt.ylim([0,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5bae90",
   "metadata": {},
   "source": [
    "### Question 3 (of last time' tutorial) -- kept for reference\n",
    "Learn the Lasso model, using the method  `sklearn.linear_model.Lasso`, and compare the results using alpha=1 or alpha=0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ddfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?sklearn.linear_model.Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e58ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=1\n",
    "lasso1 = sklearn.linear_model.Lasso(alpha=alpha)\n",
    "lasso1.fit(X,y)\n",
    "prediction = lasso1.predict(X)\n",
    "plt.scatter(X[:,1],y, label=\"data\", c='k')\n",
    "plt.plot(X[:,1], prediction, ls=\"--\", label=\"model, alpha=%.e\"%alpha)\n",
    "plt.ylim([0,3])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce752d",
   "metadata": {},
   "source": [
    "### Question 5: -- now we actually do it \n",
    "Code your own Lasso algorithm, using numpy (first, do the exercise on the Laplace prior) -- you may need sub-gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e9cd76",
   "metadata": {},
   "source": [
    "## Now we actually do this \"question 5\", but split it in parts, to guide you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b832a0",
   "metadata": {},
   "source": [
    "### Question 5.0 - unregularized regression\n",
    "\n",
    "Here we give you the code, as an example to start.\n",
    "It's very similar to the correction `TP3.2-FeatureMaps-Correction.ipynb`. You can use your own version if you prefer (you may have to adapt the rest of the code then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3067587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classLinearRegressorFullBatch():\n",
    "\n",
    "    def __init__(self, eta=0.001, maxIter=100, seed=42, verbose=True, fit_intercept=True):\n",
    "        self.eta = eta\n",
    "        self.maxIter = maxIter\n",
    "        self.seed = seed\n",
    "        self.w = None # at the start, it's undefined\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def initializeWeights(self,D):\n",
    "        np.random.seed(self.seed)\n",
    "        wparameters = (np.random.random((D,1))-0.5)/D**0.5\n",
    "        return wparameters\n",
    "\n",
    "    def fit(self, Xraw, y):\n",
    "        ## the full-batch perceptron in its cleanest form\n",
    "        ## or really we should call it LinearClassifier with least squares loss, rather than Perceptron\n",
    "        \n",
    "        N = Xraw.shape[0]\n",
    "        if self.fit_intercept==False:\n",
    "            X = Xraw\n",
    "        else:\n",
    "            X = np.hstack((np.ones((N,1)), Xraw)) # exgtended vector, with 1's as first component\n",
    "        D = X.shape[1]        \n",
    "        w0 = self.initializeWeights(D)       \n",
    "        wparameters = w0.copy()  # deep copy\n",
    "        assert(y.shape[0]==N)\n",
    "\n",
    "        trainLoss = np.zeros(self.maxIter)\n",
    "        ## it's important to ALWAYS also follow the test error \n",
    "        ## BUT here we have only a toy model, so it doesn't matter much. We could put a noiseless ground truth as test data\n",
    "        # testLoss = np.zeros(self.maxIter) \n",
    "        \n",
    "        ## the algo itself ##\n",
    "        for epoch in range(self.maxIter):\n",
    "            tempo = (X@wparameters -y) ## array of shape (N,D)\n",
    "            wparameters -= self.eta*(1./N)*(X.T @ tempo)\n",
    "            trainLoss[epoch] = (tempo**2).sum()\n",
    "            # testLoss[epoch] = ## not absolutely needed.\n",
    "        self.w = wparameters\n",
    "        return wparameters, trainLoss\n",
    "    \n",
    "    def predict(self,Xraw):\n",
    "        N = Xraw.shape[0]\n",
    "        if self.fit_intercept==False:\n",
    "            X = Xraw\n",
    "        else:\n",
    "            X = np.hstack((np.ones((N,1)), Xraw)) \n",
    "        ypredicted = np.dot(X,self.w) ## np.sign() is NOT PRESENT here !\n",
    "        return ypredicted\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        ypred = self.predict(X)\n",
    "        difference = (ypred-y)\n",
    "        diffsquared = (np.abs(difference)**2).sum()  ## DIFFERENCE HERE  !\n",
    "        rateOfCorrectClassif = 1 - diffsquared*1.0/X.shape[0]\n",
    "        return rateOfCorrectClassif\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08360b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eta=0.01  # learning rate \n",
    "MaxIter = 10000\n",
    "reg1 = classLinearRegressorFullBatch(eta=eta, seed=42, maxIter=MaxIter, fit_intercept=True) # order of parameters does not matter\n",
    "w, trainLoss = TODO # fit the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg1.score(X,y))\n",
    "print(reg1.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3aad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(trainLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "prediction = TODO # get predictions from the model \n",
    "plt.scatter(X[:,1],y, label=\"data\", c='k')\n",
    "plt.plot(X[:,1], prediction, ls=\"--\", label=\"model, alpha=%.e\"%alpha)\n",
    "plt.ylim([0,3])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215e46b",
   "metadata": {},
   "source": [
    "### Question 5.1 - L2-regularized regression (Ridge)\n",
    "\n",
    "To get started, you may code the Ridge regression, which, using GD, is trivial to implement.\n",
    "\n",
    "We assume the regularization term comes as \n",
    "\n",
    "$$+ \\frac{\\alpha}{2} ||\\vec w||_2^2,$$ where $\\vec w$ is the vector of weights, without the bias term (when it is present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ridge():\n",
    "\n",
    "    def __init__(self, alpha = 0.01, eta=0.001, maxIter=100, seed=42, verbose=True, fit_intercept=True):\n",
    "        self.alpha = alpha\n",
    "        self.eta = eta\n",
    "        self.maxIter = maxIter\n",
    "        self.seed = seed\n",
    "        self.w = None # at the start, it's undefined\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example of run code: (uncomment with select + Ctrl+/ )\n",
    "\n",
    "# alpha = 0.01\n",
    "# eta = 0.01  # learning rate \n",
    "# MaxIter = 10000\n",
    "# ridge1 = Ridge(alpha = alpha, eta=eta, maxIter=MaxIter, fit_intercept=True)\n",
    "# w, trainLoss = ridge1.fit(X,y)\n",
    "# print(ridge1.score(X,y))\n",
    "# print(ridge1.w)\n",
    "# plt.semilogy(trainLoss)\n",
    "\n",
    "# plt.figure()\n",
    "# prediction = ridge1.predict(X)\n",
    "# plt.scatter(X[:,1],y, label=\"data\", c='k')\n",
    "# plt.plot(X[:,1], prediction, ls=\"--\", label=\"model, alpha=%.e\"%alpha)\n",
    "# plt.ylim([0,3])\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91105dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadad608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d505c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a4f7eaf",
   "metadata": {},
   "source": [
    "### Question 5.2 - L1-regularized regression (Lasso) -- Naive\n",
    "\n",
    "To get started, you may code the Lasso regression naively, i.e. simply ignore the problems around $0$ and hope that everything goes well.\n",
    "\n",
    "We assume the regularization term comes as \n",
    "\n",
    "$$+ \\frac{\\alpha}{2} ||\\vec w||_1 =  \\frac{\\alpha}{2} \\sum_{d=1}^D |w_d| ,$$ where $\\vec w$ is the vector of weights, without the bias term $w_0$ (if it is present in the model).\n",
    "\n",
    "Run it and observe the train loss as a function of the epoch number. Is the Loss converging to a well-defined value ? Why is that ?\n",
    "\n",
    "Trick: plot also the last 100 values of trainLoss, rather than all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07440848",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([-1.2, -1, -0.5, 0, 1, 1.2,5])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f7d7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sign(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveLasso():\n",
    "\n",
    "    def __init__(self, alpha = 0.01, eta=0.001, maxIter=100, seed=42, verbose=True, fit_intercept=True):\n",
    "        self.alpha = alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b509d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328b335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cded18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(trainLoss[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d9f8dc",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "make your observations here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd3fbf",
   "metadata": {},
   "source": [
    "### Question 5.3 - L1-regularized regression (Lasso) -- CORRECT version\n",
    "\n",
    "### This can take time, consider it as a bonus homework exercise\n",
    "\n",
    "Now, getting inspired by the results obtained in the TD (exercise 3.4.b for instance), derive the proper gradient updates. \n",
    "\n",
    "Then write the correct Lasso regularization code, run it and check its convergence.\n",
    "\n",
    "If you'r estuck, you may consult e.g. scikit-learn source code, `https://github.com/scikit-learn/scikit-learn/blob/1f8825c8dd6238355191e3d1c98f4e4d54cfbf16/sklearn/linear_model/_cd_fast.pyx` , lines ~160 and after.\n",
    "However this is `cython` code, quite optimized to run fast, but not especially for readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3833a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ed646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectLasso():\n",
    "\n",
    "    def __init__(self, alpha = 0.01, eta=0.001, maxIter=100, seed=42, verbose=True, fit_intercept=True):\n",
    "        pass\n",
    "        ## TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7e35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d2505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336b9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
