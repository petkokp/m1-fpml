{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60245c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array([1,2,3,4])\n",
    "N=X.shape[0]\n",
    "X = np.vstack((np.ones(N), X)).T  #or, we could do :  X.reshape((N,1))  ## then you need to change the code for plots\n",
    "wGT = np.array([2, 0.1])\n",
    "y = X@wGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1],y)\n",
    "plt.ylim([0,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389cadfa",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Plot the Lasso Loss $L(\\alpha, w_0, w_1, X, y)$ as a function of the component $w_1$, for $\\alpha=0$, $\\alpha=1$ and $\\alpha=2$. Ignore the bias $b=w_0$ at first, setting it to its true value, so that there is only one parameter to vary, `w[1]`.\n",
    "\n",
    "What is appearing as you increase $\\alpha$ (besides the trivial upward shift of the curve due to adding a large regularization)?\n",
    "\n",
    "Where is the minimum of the loss ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d69647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "486685df",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Plot also the Ridge Loss for $\\lambda=1$ and $\\lambda=0.01$. Ignore the bias at first, setting it to its true value.\n",
    "\n",
    "Do you see the same thing appearing ? (besides the trivial upward shift of the curve due to adding a large regularization)\n",
    "\n",
    "Where is the minimum of the loss ? (answer with words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a91fcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f5bae90",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Learn the Lasso model, using the method  `sklearn.linear_model.Lasso`, and compare the results using alpha=1 or alpha=0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ddfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?sklearn.linear_model.Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b078991",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso1 = sklearn.linear_model.Lasso(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f63af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1],y, label=\"data\", c='k')\n",
    "plt.plot(X[:,1], prediction, ls=\"--\", label=\"model, alpha=%.e\"%alpha)\n",
    "plt.ylim([0,3])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8672a7be",
   "metadata": {},
   "source": [
    "### Note: \n",
    "We have augmented the data with the \"trick of the ones\", but it wasn't needed, because `lasso1.fit_intercept= True` , so we have two bias !\n",
    "But the lasso method of sklearn does not apply the L1 regul to the bias :)\n",
    "We could also not augment the data (and reshape it, then), or just use `lasso1.fit_intercept=False` with the augmented X... but what happens then ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fdcc5a",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "What is the limit alpha value for which we get a 0 slope (in our model) ?  You can simply try many values of alpha (use a loop !) and answer with an interval (you can narrow the interval by decimation if you like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97232ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1ce752d",
   "metadata": {},
   "source": [
    "### Question 5:\n",
    "Code your own Lasso algorithm, using numpy (first, do the exercise on the Laplace prior) -- you may need sub-gradients.\n",
    "\n",
    "-> next TP !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c1e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
